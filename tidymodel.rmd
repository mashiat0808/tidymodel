---
title: "Tidymodel"
output: 
    html_document:
      toc: true
      toc_float: true
---

# Logistic model implementation using Tidymodels

As the name suggests, the tidymodel package from RStudio helps us tidying up the data. However, the group of packages that make up tidymodels do not implement statistical models themselves. Instead, they focus on making all the tasks around fitting the model much easier. Those tasks are data pre-processing and results validation.

The basic functions of tidymodel are based on cooking processes; such as the function bake() refers to applying a trained data recipe, juice() refers to extracting finalized training set.

Today we will see how to use tidymodels to make a logistic regression model, step by step.

Firstly we will need to load some packages 

```{r warning=FALSE} 

library(tidymodels)  

# Helper packages
library(readr)       # for importing data
library(vip)         # for variable importance plots
library(glmnet)
library(ranger)

```


The dataset we will be using is uploaded on https://github.com/mashiat0808/tidymodel/blob/main/hotels.csv. You can download the data from here. 

Lets load our data into RStudio and take a peek at what the data represents. 

```{r warning=FALSE} 

hotels <- 
  read_csv('https://github.com/mashiat0808/tidymodel/blob/main/hotels.csv') %>%
  mutate_if(is.character, as.factor) 


glimpse(hotels)

```


We will build a logistic regression model to predict which hotel stays included children, and which did not. Our outcome variable children is a factor variable with two levels: with or without.

```{r warning=FALSE} 

hotels %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))

```

Initially, we can see that children were only in 8.1% of the reservations. 

##Data splitting and resampling 

Let’s reserve 25% of the hotel stays to the test set. As we know our outcome variable children is pretty imbalanced so we’ll use a stratified random sample:

```{r warning=FALSE} 
set.seed(123)
splits      <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test  <- testing(splits)

# training set proportions by children
hotel_other %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))

# test set proportions by children
hotel_test  %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))
```

For this model, we will create a single resample called a validation set. In tidymodels, a validation set is treated as a single iteration of resampling. This will be a split from the 37,500 stays from hotel_other; which were not used for testing. This split creates two new datasets:

1. the set held out for the purpose of measuring performance, called the validation set
2. the remaining data used to fit the model, called the training set.


We’ll allocate 20% of the hotel_other stays to the validation set, so our model performance metrics will be computed on a single set of 7,500 hotel stays. This amount of data should provide enough precision to be a reliable indicator.

```{r warning=FALSE} 
set.seed(234)
val_set <- validation_split(hotel_other, 
                            strata = children, 
                            prop = 0.80)
val_set
```


#Logistic Regression model

###Building The model
Since our outcome variable children is categorical, logistic regression is a good first model to work with. 
We will use a model that can perform feature selection during training. The glmnet R package estimates the logistic regression slope parameters using a penalty on the process so that less relevant predictors are driven towards a value of zero. 

We are using  the parsnip package with the glmnet engine:
```{r warning=FALSE} 
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

We set the penalty argument to tune() as a placeholder for now. This is a model parameter that we will tune to find the best value for making predictions with our data. Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.

###Creating the recipe
We will create a recipe to define the preprocessing steps we need to prepare our hotel stays data for this model. We will use a number of useful recipe steps for creating features from dates:

1. step_date() creates predictors for the year, month, and day of the week.
2. step_holiday() generates a set of indicator variables for specific holidays. 
3.step_rm() removes variables; here we’ll use it to remove the original date variable since we no longer want it in the model.

4. step_dummy() converts characters or factors (i.e., nominal variables) into one or more numeric binary model terms for the levels of the original data.

5. step_zv() removes indicator variables that only contain a single unique value (e.g. all zeros). This is important because, for penalized models, the predictors should be centered and scaled.

6. step_normalize() centers and scales numeric variables.

The recipe:
```{r warning=FALSE} 
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```


###Creating workflow

We will bundle the model and recipe into a single workflow() object to make management of the R objects easier:

```{r warning=FALSE} 
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

###Creating Grid for tuning

Before we fit this model, we need to set up a grid of penalty values to tune. 
```{r warning=FALSE} 
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

```


###Train and tune the model

We will use tune::tune_grid() to train these penalized logistic regression models. We’ll also save the validation set predictions so that diagnostic information can be available after the model fit. The area under the ROC curve will be used to quantify how well the model performs across a continuum of event thresholds.

```{r warning=FALSE} 
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 
```

This plots shows us that model performance is generally better at the smaller penalty values, which means that the majority of the predictors are important to the model. 

Our model performance seems to plateau at the smaller penalty values, so going by the roc_auc metric alone could lead us to multiple options for the “best” value for thismodel:

```{r warning=FALSE} 

lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

Ta da! There we have it. Our logistic regression model with tidymodel. 
